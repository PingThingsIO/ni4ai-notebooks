{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windows, aligned windows, and values\n",
    "\n",
    "> The term “big data” refers to data that is so large, fast or complex that it’s difficult or impossible to process using traditional methods.\n",
    "\n",
    "This tutorial offers a guide on using non-traditional methods in PredictiveGrid to work with big time series data sets.\n",
    "\n",
    "For high resolution streams a simple query such as \"Give me all of the data in this stream\" can return a volume of data large enough to overload any computing environment. This tutorial describes options for interacting with data in various ways to enable interactions with very large volumes of data.\n",
    "\n",
    "We'll describe three methods for querying data in PredictiveGrid. In practice none of these is better or worse; there is a time and a place for each. This post will explore when each is appropriate to use.\n",
    "\n",
    "### Functions used\n",
    "- `stream.values()`\n",
    "- `stream.windows()`\n",
    "- `stream.aligned_windows()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import btrdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from btrdb.utils.timez import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = btrdb.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying data\n",
    "\n",
    "To illustrate what's meant by `BIG DATA`, let's investigate the very simple task of querying data from a single stream.\n",
    "\n",
    "If you ask for all of the data in a stream, what will that yield?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = db.streams_in_collection('sunshine/PMU1', tags={'name': 'L1MAG'})\n",
    "stream = streams[0]\n",
    "print('collection:\\t', stream.collection)\n",
    "print('stream name:\\t', stream.name)\n",
    "\n",
    "# How many points is that?\n",
    "print('size:\\t\\t', round(stream.count()/1e9,2), 'billion points')\n",
    "print('volume:\\t\\t', round(stream.count()*64*2/8/1e9,2), 'gigabytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's a lot of data!\n",
    "Querying that much data will likely overload your computing environment and will likely take quite a long time to get the data back to you. \n",
    "\n",
    "***Is there a better way?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windows Queries\n",
    "\n",
    "Windows queries provide *statistical aggregates* or \"summary statistics\" of raw data points in a given time interval. A windows query will return a time series of `StatPoint` objects, which can be used to explore summary statistics of raw values over time.\n",
    "\n",
    "New to `StatPoints`? Start with the tutorial below. \n",
    "\n",
    "https://github.com/PingThingsIO/ni4ai-notebooks/blob/main/tutorials/5%20-%20Working%20with%20StatPoints.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = currently_as_ns()\n",
    "\n",
    "start, _ = stream.earliest()\n",
    "start = ns_to_datetime(start.time)\n",
    "\n",
    "end, _ = stream.latest()\n",
    "end = ns_to_datetime(end.time)\n",
    "\n",
    "window = ns_delta(days=5)\n",
    "\n",
    "start_time = datetime(start.year, start.month, start.day+1)\n",
    "statpoints = stream.windows(start_time, end, window)\n",
    "print('Runtime: %.2f seconds'%((currently_as_ns()-t0)/1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statpoints_to_dataframe(statpoints, datetime_index=True):\n",
    "    attributes = ['min','mean','max','stddev','count','time',]\n",
    "    \n",
    "    df = pd.DataFrame([[getattr(p, attr) for attr in attributes] for p, _ in statpoints],\n",
    "                     columns=attributes)\n",
    "\n",
    "    if datetime_index:\n",
    "        df['datetime'] = [ns_to_datetime(t) for t in df['time']]\n",
    "        return df.set_index('datetime')\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "df = statpoints_to_dataframe(statpoints)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "The query `stream.windows()` scanned through 18 months [!!!] of data and returned a tuple of StatPoint objects. \n",
    "\n",
    "Those 18 months are partitioned into 5-day time increments (as specified by the `window` parameter). Each StatPoint reports summary statistics of values observed during that time frame.\n",
    "\n",
    "Note that pulling all 9+ billion raw point values for the same interval would have taken MUCH longer, and would have returned more data than would have been feasible to hold in RAM. Leveraging StatPoint objects makes it feasible to mine through long time intervals of data to look for trends or event signatures that warrant more detailed / granular analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens if we zoom in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = currently_as_ns()\n",
    "\n",
    "window = ns_delta(days=1)\n",
    "statpoints = stream.windows(start, end, window)\n",
    "print('Data Duration:',(end-start))\n",
    "print('Aggregation window:', timedelta(seconds=int(window/1e9)))\n",
    "print('Runtime: %.2f seconds'%((currently_as_ns()-t0)/1e9))\n",
    "\n",
    "df = statpoints_to_dataframe(statpoints)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligned windows\n",
    "\n",
    "Aligned windows return results that look very much like windows queries. The only differece is that time stamps are adjusted to align with time windows stored inherently in the database. Where `windows` queries may need to re-compute statistical aggregates over the time window requested, `aligned_windows` queries can leverage pre-computed values.\n",
    "\n",
    "\n",
    "Let's look at the difference in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = ns_delta(days=1)\n",
    "pw = np.log2(window)\n",
    "\n",
    "t0 = currently_as_ns()\n",
    "statpoints = stream.aligned_windows(start, end, pointwidth=pw)\n",
    "print('Data Duration:',(end-start))\n",
    "print('Aggregation window:', timedelta(seconds=int(window/1e9)))\n",
    "print('Runtime: %.2f seconds'%((currently_as_ns()-t0)/1e9))\n",
    "\n",
    "df = statpoints_to_dataframe(statpoints)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much faster! The only thing to note is that the time increment in an `aligned_windows` query is rounded to the nearest time increment that matches the inherent structure of the database. This means your start time, end time, and window may be modified slightly to optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('WINDOW DURATION')\n",
    "print('\\tAs specified:', timedelta(seconds=int(window/1e9)))\n",
    "print('\\tAs returned:', btrdb.utils.general.pointwidth(pw))\n",
    "\n",
    "\n",
    "print('\\n\\nSTART TIME')\n",
    "print('\\tAs specified:', start)\n",
    "print('\\tAs returned:', df.index.min())\n",
    "\n",
    "\n",
    "print('\\n\\nEND TIME')\n",
    "print('\\tAs specified:', end)\n",
    "print('\\tAs returned:', df.index.max()+timedelta(seconds=int(window/1e9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting more granular with aligned_windows\n",
    "\n",
    "Performance on `aligned_windows` queries is much faster, and will enable you to query data more quickly and at finer resolutions that you'll be able to do using `windows`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = ns_delta(hours=6)\n",
    "pw = np.log2(window)\n",
    "\n",
    "t0 = currently_as_ns()\n",
    "statpoints = stream.aligned_windows(start, end, pointwidth=pw)\n",
    "\n",
    "print('Data Duration:',(end-start))\n",
    "print('Aggregation window:', btrdb.utils.general.pointwidth(pw))\n",
    "print('Runtime: %.2f seconds'%((currently_as_ns()-t0)/1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = ns_delta(minutes=30)\n",
    "pw = np.log2(window)\n",
    "\n",
    "t0 = currently_as_ns()\n",
    "statpoints = stream.aligned_windows(start, end, pointwidth=pw)\n",
    "\n",
    "print('Data Duration:',(end-start))\n",
    "print('Aggregation window:', btrdb.utils.general.pointwidth(pw))\n",
    "print('Runtime: %.2f seconds'%((currently_as_ns()-t0)/1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = ns_delta(minutes=1)\n",
    "pw = np.log2(window)\n",
    "\n",
    "t0 = currently_as_ns()\n",
    "statpoints = stream.aligned_windows(start, end, pointwidth=pw)\n",
    "\n",
    "print('Data Duration:',(end-start))\n",
    "print('Aggregation window:', btrdb.utils.general.pointwidth(pw))\n",
    "print('Runtime: %.2f seconds'%((currently_as_ns()-t0)/1e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That last query took a while! Let's make note that querying 1.5 years of data at 1-minute resolution is starting to push the limits of what our environment (or patience!) can handle.\n",
    "\n",
    "It is possible to speed that up, however, by using a larger computing environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use `values`\n",
    "\n",
    "Many analytics can be done using StatPoints to summarize steady state characteristics of the data at the time-scale that is of interest, or to identify intervals in the data where there is an \"event\" in the data. \n",
    "\n",
    "Here, we'll simply explore at what point values queries become intractable to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = ns_delta(minutes=1)\n",
    "start_time = datetime_to_ns(start)\n",
    "end_time = start_time + window\n",
    "\n",
    "t0 = currently_as_ns()\n",
    "statpoints = stream.values(start_time, end_time)\n",
    "print('Runtime: %.2f seconds'%((currently_as_ns()-t0)/1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = ns_delta(minutes=10)\n",
    "start_time = datetime_to_ns(start)\n",
    "end_time = start_time + window\n",
    "\n",
    "t0 = currently_as_ns()\n",
    "statpoints = stream.values(start_time, end_time)\n",
    "print('Runtime: %.2f seconds'%((currently_as_ns()-t0)/1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = ns_delta(hours=1)\n",
    "start_time = datetime_to_ns(start)\n",
    "end_time = start_time + window\n",
    "\n",
    "t0 = currently_as_ns()\n",
    "statpoints = stream.values(start_time, end_time)\n",
    "print('Runtime: %.2f seconds'%((currently_as_ns()-t0)/1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = ns_delta(hours=6)\n",
    "start_time = datetime_to_ns(start)\n",
    "end_time = start_time + window\n",
    "\n",
    "t0 = currently_as_ns()\n",
    "statpoints = stream.values(start_time, end_time)\n",
    "print('Runtime: %.2f seconds'%((currently_as_ns()-t0)/1e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final note...\n",
    "\n",
    "When running values queries, be sure to check how much working memory you have available in your jupyterhub instance. Bringing large amounts of data into memory can easily cause your environment to crash! You may need to shut down and move to a larger instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `aligned_windows` queries in action \n",
    "Here are some examples where we use statpoints to hone in on time intervals that are known (or likely) to be of interest for a given analytic:\n",
    "- Voltage sags: https://github.com/PingThingsIO/ni4ai-notebooks/blob/main/demo/Voltage%20Sag%20Exploration.ipynb\n",
    "- Tap changes: https://github.com/PingThingsIO/ni4ai-notebooks/blob/main/demo/Voltage%20Change%20Detection.ipynb\n",
    "\n",
    "### `values` queries in action\n",
    "\n",
    "Here are examples where we use values queries to examine events that warrant full-resolution queries:\n",
    "- Spectral analysis: https://github.com/PingThingsIO/ni4ai-notebooks/blob/main/demo/PV_spectrogram.ipynb\n",
    "- Phase angle differencing: https://github.com/PingThingsIO/ni4ai-notebooks/blob/main/demo/Phase%20Angle%20Monitoring.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

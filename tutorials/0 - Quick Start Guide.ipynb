{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PredictiveGrid in Python\n",
    "## A Quick Start Notebook\n",
    "\n",
    "This notebook is a quick start to working with data using PredictiveGrid's Python API. It illustrates the basic, key functionality of the API to give you a running start working with data in the platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first step is to import all the required packages. Below is a list of basic imports that you can copy and paste into your own notebooks to get going.\n",
    "The external python libraries (```matplotlib```, ```numpy```, etc.) have wonderful, extensive documentation that you should look up if you want to explore all their functionalities or just unstick yourself. \n",
    "The ```%matplotlib inline``` command ensures that plots will be rendered in the notebook itself, rather than in a pop-up window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PredictiveGrid imports\n",
    "import btrdb # Platform Python bindings\n",
    "from btrdb.utils import timez # helpful sub-package for handling time\n",
    "# from btrdb.stream import StreamSet # subpackage with light wrapper for working with multiple streams\n",
    "from btrdb.utils.general import pointwidth\n",
    "\n",
    "# support the new v6 api, currently leverage the util module from btrdb v5\n",
    "import btrdbv6\n",
    "from btrdbv6 import StreamSet\n",
    "\n",
    "# External Python libraries\n",
    "import numpy as np # scientific computing package\n",
    "import matplotlib.pyplot as plt # plotting package\n",
    "import pandas as pd # data analysis library\n",
    "from tabulate import tabulate # creating & printing neat tables\n",
    "from datetime import datetime, timedelta # for working with time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must connect to the database. The ```connect``` function optionally accepts endpoint and apikey string arguments, which can be passed like:\n",
    "\n",
    "```btrdb.connect(endpoint_string, apikey=key_string) ```\n",
    "\n",
    "When running a notebook on JupyterHub, these do not need to be passed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add in info command\n",
    "conn = btrdbv6.connect(profile=\"debbie\")\n",
    "conn\n",
    "#conn.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Collections\n",
    "The streams in the database are organized into collections, which can be thought of as heirarchical paths such as ```CALIFORNIA/SanFrancisco/91405/Sensor1``` but are internally just strings. A single collection, defined by a path-like name, can contain any number of individual streams. It is best to name collections and organize streams into them in some logically consistent and descriptive way to facilitate searching (think about creating a file heirarchy to organize data files and mirror this in the collection name). \n",
    "\n",
    "Let us query all and print out some of the collections in this cluster. We will print out the individual collection names to reflect the implicit heirarchical data organization they encode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo list available collections in db\n",
    "collections = conn.list_collections()\n",
    "print(f'Found {len(collections)} collections\\n')\n",
    "\n",
    "collections.sort()\n",
    "\n",
    "# We limit the number of collections so we don't print too many\n",
    "for ith_collection, collection_name in enumerate(collections[0:3]):\n",
    "    levels = collection_name.split('/')\n",
    "    for ith_level, level in enumerate(levels):\n",
    "        print(ith_level*' ','->', level)\n",
    "    print(\"========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also limit the search to collections with a certain prefix (you may want to change the prefix below to obtain interesting results on your particular allocation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'sunshine/PMU1'\n",
    "collections = conn.list_collections(prefix)\n",
    "\n",
    "print(f'Found {len(collections)} collections : {collections}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Streams\n",
    "Now that we have a collection, the next step is to find individual streams (though we could have done this directly, if we know what we want). As you may recall, each stream represents a single time series within the database, which contains the atomic, ```(time, value)``` pairs, and has some associated metadata. \n",
    "\n",
    "To get the streams, we will use the ```streams_in_collection``` method. This takes an optional collection prefix argument. If we pass in no argument, we will get all streams in the collection. Let us work with the collection we found previously. Note that the method returns a `StreamSet` object, which we can iterate through to see the found streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: get collection names from the above code, for now use a workaround\n",
    "# collection = collections[0]\n",
    "collection = \"justin_test_insert_bench\"\n",
    "streams = conn.streams_in_collection(collection)\n",
    "print(f'Found {len(streams)} streams in collection \"{collection}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams[0].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that streams have metadata, which can help us understand what the stream *is*. The convenience function below will obtain and pretty print a set of streams and their metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_streams(streams):\n",
    "    table = [[\"Collection\", \"Name\", \"Units\", \"Version\", \"UUID\"]]\n",
    "    for stream in streams:\n",
    "        # Get all the tags for this stream. \n",
    "        tags = stream.tags\n",
    "        table.append([\n",
    "            stream.collection, stream.name, tags[\"unit\"], stream.version, stream.uuid\n",
    "        ])\n",
    "    return tabulate(table, headers=\"firstrow\")\n",
    "\n",
    "print(describe_streams(streams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search for streams in terms of their metadata tags, by passing an optional ```tag``` arguement to the same ```streams_in_collection``` method. Let us get all the streams with units of ```volts```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo add in support for tags, annotations, etc querying\n",
    "streams = conn.streams_in_collection(collection, tags={\"unit\": \"volts\"})\n",
    "print(describe_streams(streams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API also supports SQL queries to find streams and metadata, which you can learn more about in [this notebook](https://github.com/PingThingsIO/ni4ai-notebooks/blob/main/tutorials/7%20-%20Working%20with%20Metadata.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with a stream\n",
    "Let us work with the first stream in the above list of streams. We will look at its metadata and learn how to query time series data from the stream.\n",
    "\n",
    "### NOTE\n",
    "`Stream` objects are simple objects representing relevant metadata from BTrDB, if we want to query and access data from these streams, we **must** use a `StreamSet` object, even if it is just a single stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = StreamSet.from_streams([streams[0]], db_conn=conn)\n",
    "print(stream)\n",
    "print(stream.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's checkout the stream's metadata. Note that ```Stream.collections``` returns a dict mapping stream uuids to their collection name, ```stream.tags()``` returns a dictionary of stream uuids to their tags, while ```stream.annotations()``` returns a dictionary of stream uuids mapping to their available annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'COLLECTION: {stream.collections()}')\n",
    "\n",
    "print(f'TAGS: {stream.tags()}')\n",
    "print(f'ANNOTATIONS: {stream.annotations()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uu = stream.get_uuids()\n",
    "print(uu)\n",
    "uu = uu[0]\n",
    "print()\n",
    "\n",
    "print(f'COLLECTION: {stream.collections()[uu]}')\n",
    "\n",
    "print(f'TAGS: {stream.tags()[uu]}')\n",
    "print(f'ANNOTATIONS: {stream.annotations()[uu]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types from Time Series Queries\n",
    "\n",
    "Before we move on, let's refresh some concepts on BTrDB data types.  \n",
    "\n",
    "There are two data types (represented by Python objects) that we may obtain when querying time series data from a stream. These are: \n",
    "\n",
    "1. ```RawPoint```s --- RawPoints represent the atomic data type in a time series: a paired timestamp and value that is the raw data of the series. Recall that these are stored at the absolute bottom of the BTrDB tree.\n",
    "2. ```StatPoint```s --- These are stored in the internal nodes of the BTrDB tree and contain precomputed statistical aggregates of the raw data that lies beneath them. \n",
    "\n",
    "Querying ```RawPoint```s will be slower than querying ```StatPoint```s. Between ```StatPoint```s, it is faster to query those that reside higher up in the tree, corresponding to longer durations of raw data and correspondingly larger numbers of ```RawPoint```s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Time Range\n",
    "There are a couple useful methods for understanding the time range covered by a stream. We can do this by getting the *earliest* and *latest* ```RawPoint```s in the stream, as shown below. \n",
    "\n",
    "A few things: queries for time series Points always return the Point(s) requested along with a ***version number***. For most introductory users, the version is not useful, though you can read more about it in our docs. Below, we discard the version and just work with the returned ```RawPoint```. \n",
    "\n",
    "Time stamps in BTrDB are integers which represent *nanoseconds since the Unix epoch* [(January 1, 1970)](https://www.epochconverter.com/). These can be easily converted to more readable time stamps using utility functions in the ```timez``` package [(read the docs here)](https://btrdb.readthedocs.io/en/latest/api/utils-timez.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: support stream.earliest, and latest\n",
    "# Get the earliest & latest RawPoints in the stream\n",
    "earliest = stream.earliest()\n",
    "latest = stream.latest()\n",
    "\n",
    "\n",
    "print('Stream starts at: ', timez.ns_to_datetime(earliest_time))\n",
    "print('Stream ends at: ', timez.ns_to_datetime(latest_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Data\n",
    "There are two methods to query data from a stream. These are documented & described below (in order of increasing query speed). \n",
    "1. ```StreamSet.values(start, end)```: This call will return all ```RawPoint```s in the streamset between the ```start``` and ```end``` time.\n",
    "2. ```StreamSet.statspoints(start, end, width)```: This call will return ```StatPoint```s spanning (and summarizing) windows of length ```width``` nanoseconds between the ```start``` and ```end``` times. The ```width```argument is an integer specifying the window length in *nanoseconds*.\n",
    "\n",
    "To reiterate, ```values``` returns ```RawPoint```s, while ```windows``` return ```StatPoint```s. \n",
    "\n",
    "All these queries return a numpy array of the format `timestamp_0, (stream_0 value, stream_1 value, ... stream_N-1 value, stream_N value)`, (Where the values will either be ```RawPoint``` or ```StatPoint```). Further, as mentioned, the time stamps in the Points are in nanoseconds since the Unix epoch format. \n",
    "\n",
    "Overall, we need a function to convert this returned data into a more conducive format. The user can convert these values to a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) or [numpy Array](https://numpy.org) which are easy to work with. The functions to do this are shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```stream.values```\n",
    "Let us choose a ```start``` and ```end``` time for a ```values``` query. **CAUTION**: Most streams contain very high frequency data, so don't use too long a duration when querying raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = btrdb.utils.timez.to_nanoseconds(datetime(2022, 1, 1))\n",
    "end = start + btrdb.utils.timez.ns_delta(seconds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.tags(update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the data\n",
    "stream = stream.values(start, end)\n",
    "# Convert it to a pandas Dataframe\n",
    "data = stream.to_dataframe()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very easy to plot the data in a pandas dataframe. Just call ```plot()```! Here we pass a few aesthetic arguments to the call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(linestyle='--', marker='o', figsize=(15, 5), \n",
    "          title=f'Rawvalues of stream \"{stream[0].name}\"', \n",
    "          ylabel=stream.tags()[uu]['unit']); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```stream.windows```\n",
    "Let us choose a longer time range and a window length for a ```windows``` query, which will return ```StatPoint```s. Remeber that the window width must be specified as an integer number of nanoseconds.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = btrdb.utils.timez.to_nanoseconds(datetime(2015, 10, 10))\n",
    "start = btrdb.utils.timez.to_nanoseconds(datetime(2022, 1, 1))\n",
    "end = start + btrdb.utils.timez.ns_delta(hours=24)\n",
    "# width_ns = btrdb.utils.timez.ns_delta(minutes=1)\n",
    "width_ns = btrdb.utils.timez.ns_delta(minutes=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the data\n",
    "stream.get_statspoints(start, end, width_ns)\n",
    "# Convert it to a pandas Dataframe\n",
    "data = stream.to_dataframe()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of a pandas ```DataFrame``` as an excel file. In this case, the rows are timestamps (coressponding to the windows) and the columns contain the different aggregates. \n",
    "\n",
    "We can visualize the dataframe structure and entries with the ```head()``` call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the resulting data. We can call ```.plot()``` on the ```DataFrame```, as we did earlier, but instead, we will manually plot only some of the columns, for easier and greater control of the visualization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import IndexSlice as idx\n",
    "uu = stream.get_uuids()[0]\n",
    "plt.figure(figsize=(15, 5))\n",
    "# Plot the mean of the windows\n",
    "plt.plot(data.index, data.loc[:, (idx[:, \"mean\"])], linewidth=2, color='red', linestyle='--', marker='o')\n",
    "# Show the range of the minimum and maximum over the windows\n",
    "plt.fill_between(data.index, data.loc[:, (idx[:, \"min\"])].values.flatten(), data.loc[:, (idx[:, \"max\"])].values.flatten(), color='lightgrey')\n",
    "plt.ylabel(stream.tags()[uu]['unit'])\n",
    "plt.title(f'Mean, Min and Max of stream \"{stream.tags()[uu][\"name\"]}\" using windows() with width=1min');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with multiple streams using streamsets\n",
    "We often want to query data from a bunch of streams. Of course, we could do this manually by iterating through the individual streams and using the methods described above. However, we also have the option of using a [```StreamSet```](https://btrdb.readthedocs.io/en/latest/working/streamsets.html), which is a light wrapper around a list of streams. \n",
    "\n",
    "The following cells show a few examples for working with ```StreamSet```s. For more, see [this notebook](https://github.com/PingThingsIO/ni4ai-notebooks/blob/main/tutorials/3%20-%20Working%20With%20StreamSets.ipynb). Notice that when querying ```StreamSet```s, version numbers are no longer returned, as they were for queries on single streams. \n",
    "\n",
    "To begin, we use the same collection as before, and get a list of streams, which we then convert to a ```StreamSet```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = conn.streams_in_collection(collection, tags={\"unit\": \"volts\"})\n",
    "streamset = StreamSet(streams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do some queries on all streams at once. For example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earliests = streamset.earliest() # Returns a list of RawPoints, one for each stream\n",
    "latests = streamset.latest() # Returns a list of RawPoints, one for each stream\n",
    "\n",
    "for i in range(len(streamset)):\n",
    "    # Get the times for this stream\n",
    "    earliest_time = earliests[i].time\n",
    "    latest_time = latests[i].time\n",
    "    \n",
    "    print(f'Stream {streamset[i].name}')\n",
    "    print(f'starts at: {timez.ns_to_datetime(earliest_time)}')\n",
    "    print(f'ends at: {timez.ns_to_datetime(latest_time)}')\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make a ```values``` (ie raw data) query on this ```StreamSet```. Notice the difference in the query form: we use a function chaining approach to first set the ```start``` and ```end``` times. Then, the ```values``` are requested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2015, 10, 10)\n",
    "end = start + timedelta(seconds=5)\n",
    "data = streamset.filter(start, end).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result ```data```, is a list of lists, each containing the ```RawPoint```s for one of the streams in the ```StreamSet```. \n",
    "\n",
    "To make the data more workable, we can instead query the result as a pandas ```DataFrame```, as shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = streamset.filter(start, end).to_dataframe()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The timestamps still need to be converted, which we do below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert nanoseconds to human readable time\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(figsize=(15, 5), linewidth=3, ylabel='volts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is it! You've had a basic introduction to working with PredictiveGrid in Python. You are ready to start working with your data and conducting some interesting analyses.\n",
    "\n",
    "As you go, you will want to learn about and use more advanced platform capabilities. Make sure to look at the notebooks [here](https://github.com/PingThingsIO/ni4ai-notebooks) containing both tutorials and analytics demos. You can also [read the blog](https://blog.ni4ai.org/), which shares user stories, capabilities, etc. Also, read the docs! \n",
    "\n",
    "# THE END #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

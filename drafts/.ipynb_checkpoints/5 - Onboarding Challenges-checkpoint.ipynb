{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with PingThings Data Analytics\n",
    "\n",
    "Welcome to NI4AI! This notebook is designed to help you learn to use the PredictiveGrid platform more effectively by giving you some common code challenges that will allow you to encounter typical day-to-day problems that data scientists working in power systems may face. We encourage you to work through this notebook. Feel free to reach out to info@ni4ai.org with questions if you have them. \n",
    "\n",
    "This notebook is broken up into three sections:\n",
    "\n",
    "1. Working with the BTrDB Database\n",
    "2. Performing time series analyses\n",
    "3. Performing power engineering computations\n",
    "\n",
    "Where possible, I've provided some reading material that may help your understanding. We can update this notebook as needed to further refine it based on your questions! \n",
    "\n",
    "## Setting up your Environment\n",
    "\n",
    "Before you get started, you will need the following things:\n",
    "\n",
    "1. Access to an _allocation_ (e.g. argon, ni4ai, husky, dominion, etc)\n",
    "2. An API key (and a username and password for the plotter and JupyterHub) \n",
    "3. Access to the [chained-library](https://github.com/pingThingsIO/chained-library) GitHub repository (optional)\n",
    "\n",
    "**Copy this notebook into a user directory before working on it, please don't modify the base notebook!**\n",
    "\n",
    "If you have access to JupyterHub, I recommend cloning this repository there first and working in the cluster rather than locally. \n",
    "\n",
    "If you don't have access to JupyterHub, make sure you install all dependencies using:\n",
    "\n",
    "```\n",
    "$ pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Also, be sure to set up your environment variables correctly, either using `pgops env` or by adding the following environment variables to your bash profile:\n",
    "\n",
    "1. `$BTRDB_ENDPOINTS`\n",
    "2. `$BTRDB_API_KEY`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with BTrDB\n",
    "\n",
    "### Task 0: Connect to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import btrdb \n",
    "\n",
    "db = btrdb.connect()\n",
    "db.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Finding Streams\n",
    "\n",
    "Perform the following tasks in your allocation:\n",
    "\n",
    "1. Print a table that lists collections and number of streams per collection\n",
    "2. Print a table that lists the streams in the collection, along with name, unit, and other metadata \n",
    "3. Use `db.query` to refactor your code from steps 1 and 2 (does it increase performance?)\n",
    "4. Print a table that reports for a collection: for each stream, the earliest and latest time stamps, and the number of points per stream \n",
    "5. Print a table of collections with number of streams, earliest and latest point and number of points per collection\n",
    "\n",
    "Checkout the [tabulate](https://pypi.org/project/tabulate/) python package for printing nice tables; you can also use ipython html display. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: CRUD Operations \n",
    "\n",
    "If you have the permissions to do so, in a `test/yourname` collection perform the following tasks:\n",
    "\n",
    "1. Create a stream\n",
    "2. Add some annotations to the stream \n",
    "3. Generate some [random walk data](https://machinelearningmastery.com/gentle-introduction-random-walk-times-series-forecasting-python/) and insert it into your stream \n",
    "4. Update your stream's annotations with the parameters of your random walk \n",
    "5. Delete data from the middle of the stream\n",
    "6. Read the original data without the insertion using a version number \n",
    "7. Insert data into the middle of the stream, overwriting points \n",
    "8. Create a diff of the the version created in 7 vs the version before 5 \n",
    "9. Obliterate your stream\n",
    "\n",
    "_Please only perform these operations in a development allocation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Frequency and Gap Detection \n",
    "\n",
    "This task requires you to create a stream with the following characteristics (following from the random walk data generator from task 2):\n",
    "\n",
    "1. This stream must be in the `test/yourname` collection\n",
    "2. It should contain data spanning at least 3 hours\n",
    "3. It should start at 30Hz, go to 60Hz, down to 15Hz at three discrete points\n",
    "4. It should contain missing data in each frequency phase, including several 200ms-800ms gaps, 1-5s gaps, 45s-5m gaps, and 10-30m gaps \n",
    "\n",
    "You can create different streams with different characteristics as above if that's easier. With these streams, write algorithms that perform the following analyses (without information from your data generator):\n",
    "\n",
    "1. For a specified pointwidth, determine when the sample rates change (e.g. when there was a configuration change)\n",
    "2. Detect the start and end of the gaps in the data\n",
    "3. For a specified width, detect windows that have less than the expected number of points\n",
    "\n",
    "\n",
    "Make sure you obliterate your stream when you are done!\n",
    "\n",
    "_Please only perform these operations in a development allocation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Working with Pandas\n",
    "\n",
    "Select stream(s) from the allocation you're using, voltage or current magnitudes are probably your best bet.\n",
    "\n",
    "1. Write a function that queries raw values and returns a pandas Series where the index is a pd.Timestamp \n",
    "2. Update the above function to accept possibly multiple streams as input and return a DataFrame \n",
    "3. Use interpolation to fill in `np.nan` values in the above DataFrame\n",
    "4. Use timestamp truncation to fill in the `np.nan` values in the above DataFrame \n",
    "5. Write a new function that returns a DataFrame for stat points of a stream using an aligned_windows or windows query\n",
    "6. Update the above to just get the means or another aggregate function for multiple streams to create a DataFrame from a windows query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Visualization\n",
    "\n",
    "Select stream(s) from the allocation you're using, voltage or current magnitudes are probably your best bet.\n",
    "\n",
    "1. Using matplotlib and a windows or aligned windows query, plot the mean series with a lighter fill area between the minimum and the maximum (e.g. similar to the plotter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
